# tf-NMT
seq2seq(RNN Encoder-Decoder architectures + Attention mechanism) learning Using TensorFlow.

The code was implemented using the tf.contrib.seq2seq modules

AttentionWrapper

Decoder

BasicDecoder

LSTM

AdamOptimizer

# Dependencies
Tensorflow = 1.4.1

python = 3.6

# Acknowledgements
[tensorflow/nmt](https://github.com/tensorflow/nmt): tensorflow official seq2seq tutorial

[deepschool.io/Seq2Seq](https://github.com/sachinruk/deepschool.io/blob/master/Lesson%2019%20-%20Seq2Seq%20-%20Date%20translator%20-%20Solutions.ipynb): good simple example about seq2seq architectures

[llSourcell/seq2seq_model_live](https://github.com/llSourcell/seq2seq_model_live/blob/master/2-seq2seq-advanced.ipynb): Nice explanation about seq2seq architectures

[tf_tutorial_plus](https://github.com/j-min/tf_tutorial_plus): Nice tutorials for tf.contrib.seq2seq API

